
#  Do the following on OCI Console/CLI, etc before running this script. 
#  Create Dynamic Group
#  nfs_ha - DG for creating HA Active-Passive node pairs
#  ANY {instance.compartment.id =  'ocid1.compartment.oc1..aaaaaaaawiovn4frspcslaqkf4glwndsvf4obwjdunsf5paj4qv6f4opls7a'}
#  Create policies
#  Allow dynamic-group nfs_ha to use private-ips in compartment pinkesh
#  Allow dynamic-group nfs_ha to use vnics in compartment pinkesh



SSH_OPTIONS=" -i /home/opc/.ssh/id_rsa -o BatchMode=yes -o StrictHostkeyChecking=no "
MDATA_VNIC_URL="http://169.254.169.254/opc/v1/vnics/"
pcs_cfg="nfs_cfg"
lv_name="{{ lv_name }}"
vg_name="{{ vg_name }}"
ipaddr2_vip_name="{{ ipaddr2_vip_name }}"
service_name="nfs-daemon"
filesystem_name="nfsshare"
filesystem_type="xfs"
filesystem_device="/dev/{{ vg_name }}/{{ lv_name }}"
filesystem_dir="/mnt/nfsshare"

group_name="nfsgroup"
pcs_group_parameter=" --group ${group_name}"

#exportfs_client_spec_1_cidr_block="10.0.3.0/24"
#exportfs_client_spec_2_cidr_block="10.0.0.0/24"


exportfs_client_spec_1_cidr_block="{{ private_storage_subnet_cidr_block }}"
exportfs_client_spec_2_cidr_block="{{ public_subnet_cidr_block }}"
exportfs_client_spec_1_netmask=`ipcalc -m  $exportfs_client_spec_1_cidr_block | gawk -F "=" '{ print $2}'`
exportfs_client_spec_1_network=`ipcalc -n  $exportfs_client_spec_1_cidr_block | gawk -F "=" '{ print $2}'`
exportfs_client_spec_2_netmask=`ipcalc -m  $exportfs_client_spec_2_cidr_block | gawk -F "=" '{ print $2}'`
exportfs_client_spec_2_network=`ipcalc -n  $exportfs_client_spec_2_cidr_block | gawk -F "=" '{ print $2}'`
exportfs_client_spec_1="${exportfs_client_spec_1_network}/${exportfs_client_spec_1_netmask}"
exportfs_client_spec_2="${exportfs_client_spec_2_network}/${exportfs_client_spec_2_netmask}"
options="rw,sync,no_root_squash,no_all_squash,no_subtree_check,insecure_locks"
nfs_root_dir="${filesystem_dir}/exports"
nfs_export1_dir="${nfs_root_dir}/export1"

  if [ "{{ standard_storage_node_dual_nics }}" = "true" ]; then
    exportfs_client_spec_3_cidr_block="{{ private_fs_subnet_cidr_block }}"
    exportfs_client_spec_3_netmask=`ipcalc -m  $exportfs_client_spec_3_cidr_block | gawk -F "=" '{ print $2}'`
    exportfs_client_spec_3_network=`ipcalc -n  $exportfs_client_spec_3_cidr_block | gawk -F "=" '{ print $2}'`
    exportfs_client_spec_3="${exportfs_client_spec_3_network}/${exportfs_client_spec_3_netmask}"
  fi

ha_config_dir="/home/opc/ha_config"





function configure_vip_move {

  # use Instance Principal for auth
  # Configuring OCI-CLI
  mkdir /home/oracle-cli/
  chown root: /home/oracle-cli/
  chmod 755 /home/oracle-cli/
  wget https://raw.githubusercontent.com/oracle/oci-cli/master/scripts/install/install.sh
  bash install.sh --accept-all-defaults --exec-dir /home/oracle-cli/bin/ --install-dir /home/oracle-cli/lib/
  rm -f install.sh
  rm -rf /root/bin/oci-cli-scripts
  mkdir /home/oracle-cli/.oci
  chown hacluster:haclient /home/oracle-cli/.oci
  chmod 700 /home/oracle-cli/.oci
  /home/oracle-cli/bin/oci os ns get --auth instance_principal

  cp ${ha_config_dir}/move_secip.sh /home/oracle-cli/move_secip.sh
  chmod +x /home/oracle-cli/move_secip.sh
  chmod 700 /home/oracle-cli/move_secip.sh
  chown hacluster:haclient /home/oracle-cli/move_secip.sh
  cp -f /root/env_variables.sh /home/oracle-cli/
  chown hacluster:haclient /home/oracle-cli/env_variables.sh

  cp ${ha_config_dir}/ip_move.sh   /var/lib/pacemaker/ip_move.sh
  chown root: /var/lib/pacemaker/ip_move.sh
  chmod 0755 /var/lib/pacemaker/ip_move.sh
  sed -i "s|IPADDR2_VIP_NAME|${IPADDR2_VIP_NAME}|g" /var/lib/pacemaker/ip_move.sh
  touch /var/log/pacemaker_ip_move.log
  chown hacluster:haclient /var/log/pacemaker_ip_move.log

}


  echo "Entering Shared Disk HA Script"

  #Deploy components to implement HA for Storage Service

  LOCAL_NODE=`hostname`;
  LOCAL_NODE_IP=`nslookup $LOCAL_NODE | grep "Address: " | grep -v "#" | gawk '{print $2}'` ;
  NODE1="{{ storage_server_hostname_prefix }}1" ;
  NODE2="{{ storage_server_hostname_prefix }}2" ;
  NODE1_IP=`nslookup $NODE1 | grep "Address: " | grep -v "#" | gawk '{print $2}'` ;
  NODE2_IP=`nslookup $NODE2 | grep "Address: " | grep -v "#" | gawk '{print $2}'` ;
  NODE1_FQDN="{{ storage_server_hostname_prefix }}1.{{ storage_subnet_domain_name }}" ;
  NODE2_FQDN="{{ storage_server_hostname_prefix }}2.{{ storage_subnet_domain_name }}" ;
  echo "$NODE1_IP $NODE1_FQDN $NODE1" >> /etc/hosts
  echo "$NODE2_IP $NODE2_FQDN $NODE2" >> /etc/hosts
  # VIRTUAL IP
  TARGET_VIP={{ nfs_server_ip }}

  QUORUM_NODE="{{ quorum_server_hostname }}"


  if [ "$LOCAL_NODE" = "$NODE1" ]; then
    if [ "{{ standard_storage_node_dual_nics }}" = "true" ]; then
      node1vnic=`curl -s $MDATA_VNIC_URL | jq '.[1].vnicId' | sed 's/"//g' ` ;
    else
      node1vnic=`curl -s $MDATA_VNIC_URL | jq '.[0].vnicId' | sed 's/"//g' ` ;
    fi

    ssh ${SSH_OPTIONS}  opc@${NODE2_IP} "ls -l /home/opc/.ssh/id_rsa"
    while [ $? -ne 0 ]
    do
      echo "wait for TF scripts to copy ssh keys..."
      sleep 5s
      ssh ${SSH_OPTIONS}  opc@${NODE2_IP} "ls -l /home/opc/.ssh/id_rsa"
    done

    if [ "{{ standard_storage_node_dual_nics }}" = "true" ]; then
      node2vnic_w_quotes=`ssh ${SSH_OPTIONS} opc@${NODE2_IP} "curl -s $MDATA_VNIC_URL | jq '.[1].vnicId'  "` ;
    else
      node2vnic_w_quotes=`ssh ${SSH_OPTIONS} opc@${NODE2_IP} "curl -s $MDATA_VNIC_URL | jq '.[0].vnicId'  "` ;
    fi
    node2vnic=`echo $node2vnic_w_quotes |  sed 's/"//g' ` ;
  else
    # SWAP logic, since its node2 here.
    if [ "{{ standard_storage_node_dual_nics }}" = "true" ]; then
      node2vnic=`curl -s $MDATA_VNIC_URL | jq '.[1].vnicId' | sed 's/"//g' ` ;
    else
      node2vnic=`curl -s $MDATA_VNIC_URL | jq '.[0].vnicId' | sed 's/"//g' ` ;
    fi


    ssh ${SSH_OPTIONS}  opc@${NODE1_IP} "ls -l /home/opc/.ssh/id_rsa"
    while [ $? -ne 0 ]
    do
      echo "wait for TF scripts to copy ssh keys..."
      sleep 5s
      ssh ${SSH_OPTIONS}  opc@${NODE1_IP} "ls -l /home/opc/.ssh/id_rsa"
    done

    if [ "{{ standard_storage_node_dual_nics }}" = "true" ]; then
      node1vnic_w_quotes=`ssh ${SSH_OPTIONS} opc@${NODE1_IP} "curl -s $MDATA_VNIC_URL | jq '.[1].vnicId'  "` ;
    else
      node1vnic_w_quotes=`ssh ${SSH_OPTIONS} opc@${NODE1_IP} "curl -s $MDATA_VNIC_URL | jq '.[0].vnicId'  "` ;
    fi
    node1vnic=`echo $node1vnic_w_quotes |  sed 's/"//g' ` ;
  fi
  subnetCidrBlock=`curl -s $MDATA_VNIC_URL | jq '.[0].subnetCidrBlock  ' | sed 's/"//g' ` ;
  cidr_netmask=`echo $subnetCidrBlock | gawk -F"/" '{ print $2 }'` ;


  echo "
  LOCAL_NODE=\"${LOCAL_NODE}\"
  LOCAL_NODE_IP=\"${LOCAL_NODE_IP}\"
  NODE1=\"${NODE1}\"
  NODE2=\"${NODE2}\"
  NODE1_IP=\"${NODE1_IP}\"
  NODE2_IP=\"${NODE2_IP}\"
  NODE1_FQDN=\"${NODE1_FQDN}\"
  NODE2_FQDN=\"${NODE2_FQDN}\"
  TARGET_VIP=\"${TARGET_VIP}\"
  node1vnic=\"${node1vnic}\"
  node2vnic=\"${node2vnic}\"
  QUORUM_NODE=\"${QUORUM_NODE}\"
  " > /root/env_variables.sh
  echo "source /root/env_variables.sh" >>  /root/.bash_profile



  # Set password for hacluster user on both nodes
  echo -e "{{ hacluster_user_password }}\n{{ hacluster_user_password }}" | passwd hacluster
  if [ $? -ne 0 ]; then
    echo "Setting password value of {{ hacluster_user_password }} for hacluster failed"
    exit 1;
  fi


  # copy ssh private key to both oss nodes.
  if [ "$LOCAL_NODE" = "$NODE1" ]; then
    corosync-keygen -l
    cp -av /etc/corosync/authkey /home/opc/authkey
    chown opc: /home/opc/authkey

    scp ${SSH_OPTIONS}  -p -3 /home/opc/authkey opc@${NODE2_IP}:/home/opc/authkey

    ssh ${SSH_OPTIONS}  opc@${NODE2_IP} "sudo mv /home/opc/authkey /etc/corosync/authkey"
    while [ $? -ne 0 ]
    do
      echo "sleeping for 20s, so NODE2 can finish deploying corosync and create directories....."
      sleep 20s
      ssh ${SSH_OPTIONS} opc@${NODE2_IP} "sudo mv /home/opc/authkey /etc/corosync/authkey"
    done
    ssh ${SSH_OPTIONS} opc@${NODE2_IP} "sudo chown root: /etc/corosync/authkey"
    rm -f /home/opc/authkey
  fi

  cp ${ha_config_dir}/corosync.conf /etc/corosync/
  chown root: /etc/corosync/corosync.conf
  sed -i "s/LOCAL_NODE_IP/${LOCAL_NODE_IP}/" /etc/corosync/corosync.conf
  sed -i "s/NODE1/${NODE1}/" /etc/corosync/corosync.conf
  sed -i "s/NODE2/${NODE2}/" /etc/corosync/corosync.conf

  mkdir /etc/corosync/service.d/
  cp -f ${ha_config_dir}/pcmk /etc/corosync/service.d/pcmk
  chown root: /etc/corosync/service.d/pcmk

  mv -vn /etc/sysconfig/corosync /etc/sysconfig/corosync.orig
  cp -f ${ha_config_dir}/corosync  /etc/sysconfig/corosync
  chown root: /etc/sysconfig/corosync

  mkdir /var/log/corosync/
  # Primary reason for this while loop, is to ensure node2 get the authkey file from node1. Until then, Node2 should not start corosync service.
  if [ "$LOCAL_NODE" = "$NODE2" ]; then
    while ( ! [ -f /etc/corosync/authkey ] )
    do
      echo "wait for /etc/corosync/authkey to get transfer from node1"
      sleep 5s
    done
  fi

  systemctl start corosync
  if [ "$LOCAL_NODE" = "$NODE1" ]; then
    ssh ${SSH_OPTIONS} opc@${NODE2_IP}  'sudo systemctl status  corosync  | grep "(running)" ' ;
    while ( [ $? -ne 0 ] )
    do
      echo "waiting for corosync to come online on node2"
      sleep 10
      ssh ${SSH_OPTIONS} opc@${NODE2_IP}  'sudo systemctl status  corosync  | grep "(running)" ' ;
    done
  fi

  corosync-cmapctl | grep members


  # Configuring PCSD
  systemctl start pcsd.service
  if [ "$LOCAL_NODE" = "$NODE1" ]; then
    ssh ${SSH_OPTIONS} opc@${NODE2_IP}  'sudo systemctl status  pcsd  | grep "(running)" ' ;
    while ( [ $? -ne 0 ] )
    do
      echo "waiting for pcsd.service to come online on node2"
      sleep 10
      ssh ${SSH_OPTIONS} opc@${NODE2_IP}  'sudo systemctl status  pcsd  | grep "(running)" ' ;
    done
  fi

  if [ "$LOCAL_NODE" = "$NODE1" ]; then
    echo "{{ hacluster_user_password }}" | pcs cluster auth --name nfs_cluster ${NODE1} ${NODE2} -u hacluster

    echo "{{ hacluster_user_password }}" | pcs cluster auth --name nfs_cluster ${QUORUM_NODE} -u hacluster

  fi

  # call function
  configure_vip_move




  systemctl enable pcsd;
  systemctl enable pacemaker;
  systemctl enable corosync;

  # Start - Stonith SBD fencing config
  if [ "$LOCAL_NODE" = "$NODE1" ]; then

    sbd -d /dev/oracleoci/oraclevdb create
    # I should see the below info
    sbd -d /dev/oracleoci/oraclevdb dump

    ##==Dumping header on disk /dev/oracleoci/oraclevdb
    ##Header version     : 2.1
    ##UUID               : d484107e-3491-4a1d-b1ce-ea0a2ee2b649
    ##Number of slots    : 255
    ##Sector size        : 512
    ##Timeout (watchdog) : 5
    ##Timeout (allocate) : 2
    ##Timeout (loop)     : 1
    ##Timeout (msgwait)  : 10
    ##==Header on disk /dev/oracleoci/oraclevdb is dumped

  fi

  echo softdog > /etc/modules-load.d/watchdog.conf
  systemctl restart systemd-modules-load
  lsmod | egrep "(wd|dog)"

  sed -i "s/#SBD_DEVICE=.*/SBD_DEVICE=\"\/dev\/oracleoci\/oraclevdb\"/g"  /etc/sysconfig/sbd
  sed -i "s/SBD_OPTS=/SBD_OPTS=\"-W\"/g" /etc/sysconfig/sbd
  less /etc/sysconfig/sbd |  egrep "SBD_OPTS|SBD_DEVICE|SBD_DELAY_START"

  systemctl enable sbd
  # End - Stonith SBD fencing config


  cd /root;
  if [ "$LOCAL_NODE" = "$NODE1" ]; then

    # NOTE: With Stonith SBD fencing - first time starting cluster using pcs cluster start --all fails, but subsequent calls works. Similarly sometimes first time call to "pcs cluster start" fails, but subsequent calls work.  Hence the while loop.  "Don't know why.  Have reached LINBIT community.
    pcs cluster start
    while ( [ $? -ne 0 ] )
    do
      sudo pcs cluster stop --force
      echo "waiting for pcs cluster start to come online on $NODE1"
      sleep 15
      pcs cluster start
    done

    sleep 10s

    ssh ${SSH_OPTIONS} opc@${NODE2_IP}  'sudo pcs cluster start' ;
    while ( [ $? -ne 0 ] )
    do
      sudo pcs cluster stop --force
      echo "waiting for pcs cluster start to come online on $NODE2"
      sleep 10
      ssh ${SSH_OPTIONS} opc@${NODE2_IP}  'sudo pcs cluster start' ;
    done
    sleep 15s;

    pcs status

    pcs cluster cib /root/${pcs_cfg}

    # Quorum node changes
    echo "starting quorum changes..."
    pcs -f /root/${pcs_cfg} quorum device add model net host=${QUORUM_NODE} algorithm=ffsplit
    pcs -f /root/${pcs_cfg}  quorum config
    pcs -f /root/${pcs_cfg}  quorum status
    pcs -f /root/${pcs_cfg}  quorum device status
    echo "ending quorum changes."
    # end

    #pcs -f /root/${pcs_cfg} property set stonith-enabled=false
    pcs -f /root/${pcs_cfg} property set stonith-enabled=true
    pcs -f /root/${pcs_cfg} property set stonith-watchdog-timeout=10
    # In 2 nodes, there is no quorum. hence no-quorum-policy=ignore  instead of no-quorum-policy=stop
    # With 3rd Quorum node - set no-quorum-policy=stop
    pcs -f /root/${pcs_cfg} property set no-quorum-policy=stop
    # Do not change the resource-stickiness value from 100. Using value like 200 resulted in cluster not fencing/stonithing the node which is network isolated during split brain test.
    pcs -f /root/${pcs_cfg} resource defaults resource-stickiness=100
    pcs -f /root/${pcs_cfg} resource defaults migration-threshold=1


    pcs -f /root/${pcs_cfg} stonith create sbd_fencing_$NODE1  fence_sbd devices=/dev/oracleoci/oraclevdb pcmk_host_list=$NODE1 pcmk_delay_base=0
    # A 5 seconds delay was added to NODE2 to ensure there is no race condition - fence-race.
    pcs -f /root/${pcs_cfg} stonith create sbd_fencing_$NODE2  fence_sbd devices=/dev/oracleoci/oraclevdb pcmk_host_list=$NODE2 pcmk_delay_base=5

    pcs -f /root/${pcs_cfg} constraint location sbd_fencing_$NODE1 avoids $NODE1
    pcs -f /root/${pcs_cfg} constraint location sbd_fencing_$NODE2 avoids $NODE2

    pcs -f /root/${pcs_cfg} resource create $lv_name ocf:heartbeat:LVM-activate vgname=$vg_name vg_access_mode=system_id $pcs_group_parameter

    pcs -f /root/${pcs_cfg} resource create ${filesystem_name} Filesystem \
    device="${filesystem_device}" \
    directory="${filesystem_dir}" \
    fstype="${filesystem_type}" \
    options="noatime,inode64,nobarrier,uquota,prjquota" \
    $pcs_group_parameter

    pcs -f /root/${pcs_cfg} resource create ${service_name}  nfsserver \
    nfs_shared_infodir=${filesystem_dir}/nfsinfo nfs_no_notify=true \
    $pcs_group_parameter

    pcs -f /root/${pcs_cfg} resource create nfs-root_client_spec_1 exportfs \
    --after nfs-daemon \
    clientspec=${exportfs_client_spec_1} \
    options=${options} \
    directory=${nfs_root_dir} \
    fsid=0  $pcs_group_parameter

    # We need to use  " --force" to allow the same root folder to be exported for multiple subnets using fsid=0
    # Error: Value '0' of option 'fsid' is not unique across 'ocf:heartbeat:exportfs' resources. Following resources are configured with the same value of the instance attribute: 'nfs-root_client_spec_1', use --force to override
    pcs -f /root/${pcs_cfg} resource create nfs-root_client_spec_2 exportfs \
    --after nfs-root_client_spec_1 \
    clientspec=${exportfs_client_spec_2} \
    options=${options} \
    directory=${nfs_root_dir} \
    fsid=0  $pcs_group_parameter  --force

    pcs -f /root/${pcs_cfg} resource create export1_client_spec_1 exportfs \
    --after nfs-root_client_spec_2  \
    clientspec=${exportfs_client_spec_1} \
    options=${options} \
    directory=${nfs_export1_dir} \
    fsid=1 $pcs_group_parameter  --force


    pcs -f /root/${pcs_cfg} resource create export1_client_spec_2 exportfs \
    --after export1_client_spec_1 \
    clientspec=${exportfs_client_spec_2} \
    options=${options} \
    directory=${nfs_export1_dir} \
    fsid=1  $pcs_group_parameter  --force

    if [ "{{ standard_storage_node_dual_nics }}" = "true" ]; then

       pcs -f /root/${pcs_cfg} resource create nfs-root_client_spec_3 exportfs \
       --after export1_client_spec_2 \
       clientspec=${exportfs_client_spec_3} \
       options=${options} \
       directory=${nfs_root_dir} \
       fsid=0  $pcs_group_parameter  --force

       pcs -f /root/${pcs_cfg} resource create export1_client_spec_3 exportfs \
       --after nfs-root_client_spec_3 \
       clientspec=${exportfs_client_spec_3} \
       options=${options} \
       directory=${nfs_export1_dir} \
       fsid=1  $pcs_group_parameter  --force

    fi


    pcs -f /root/${pcs_cfg} resource create ${ipaddr2_vip_name} ocf:heartbeat:IPaddr2 ip=${TARGET_VIP} cidr_netmask=${cidr_netmask} op monitor interval=20s $pcs_group_parameter

    pcs -f /root/${pcs_cfg} alert create id=ip_move description="Move IP address using oci-cli" path=/var/lib/pacemaker/ip_move.sh
    pcs -f /root/${pcs_cfg} alert recipient add ip_move id=logfile_ip_move value=/var/log/pacemaker_ip_move.log

    pcs -f /root/${pcs_cfg} constraint colocation add ${filesystem_name} with ${lv_name} INFINITY
    pcs -f /root/${pcs_cfg} constraint order ${lv_name} then ${filesystem_name}

    # pcs constraint colocation add disk with ${filesystem_name} INFINITY
    # pcs constraint order disk then ${filesystem_name}

    pcs -f /root/${pcs_cfg} constraint colocation add ${service_name} with ${filesystem_name} INFINITY
    pcs -f /root/${pcs_cfg} constraint order ${filesystem_name} then ${service_name}


    pcs -f /root/${pcs_cfg} constraint colocation add nfs-root_client_spec_1 with ${service_name} INFINITY
    pcs -f /root/${pcs_cfg} constraint order ${service_name} then nfs-root_client_spec_1
    pcs -f /root/${pcs_cfg} constraint colocation add nfs-root_client_spec_2 with nfs-root_client_spec_1 INFINITY
    pcs -f /root/${pcs_cfg} constraint order nfs-root_client_spec_1 then nfs-root_client_spec_2

    pcs -f /root/${pcs_cfg} constraint colocation add export1_client_spec_1 with nfs-root_client_spec_2 INFINITY
    pcs -f /root/${pcs_cfg} constraint order nfs-root_client_spec_2 then export1_client_spec_1
    pcs -f /root/${pcs_cfg} constraint colocation add export1_client_spec_2 with export1_client_spec_1 INFINITY
    pcs -f /root/${pcs_cfg} constraint order export1_client_spec_1 then export1_client_spec_2

    if [ "{{ standard_storage_node_dual_nics }}" = "true" ]; then
      pcs -f /root/${pcs_cfg} constraint colocation add nfs-root_client_spec_3 with export1_client_spec_2 INFINITY
      pcs -f /root/${pcs_cfg} constraint order export1_client_spec_2 then nfs-root_client_spec_3
      pcs -f /root/${pcs_cfg} constraint colocation add export1_client_spec_3 with nfs-root_client_spec_3 INFINITY
      pcs -f /root/${pcs_cfg} constraint order nfs-root_client_spec_3 then export1_client_spec_3
      pcs -f /root/${pcs_cfg} constraint colocation add ${ipaddr2_vip_name} with export1_client_spec_3 INFINITY
      pcs -f /root/${pcs_cfg} constraint order export1_client_spec_3 then ${ipaddr2_vip_name}
    else
      pcs -f /root/${pcs_cfg} constraint colocation add ${ipaddr2_vip_name} with export1_client_spec_2 INFINITY
      pcs -f /root/${pcs_cfg} constraint order export1_client_spec_2 then ${ipaddr2_vip_name}
    fi

    # OLD one without exportfs
    ##pcs -f /root/${pcs_cfg} constraint colocation add ${ipaddr2_vip_name} with ${service_name} INFINITY
    ##pcs -f /root/${pcs_cfg} constraint order ${service_name} then ${ipaddr2_vip_name}


  # end of if loop node = node1
  fi

  # Now push the configuration so it becomes active.
  if [ "$LOCAL_NODE" = "$NODE1" ]; then
    pcs cluster cib-push /root/${pcs_cfg}
    echo "sleeping for 25 seconds for PCS cluster cib-push /root/${pcs_cfg} to complete"
    sleep 25s

    pcs status
    pcs stonith sbd status --full
    pcs stonith show --full
    pcs property show
    pcs resource show --full
    pcs quorum status
    pcs stonith list
    #pcs stonith describe  fence_sbd
    pcs property --all  | egrep "stonith-|watchdog"

    # Changes for Quorum node
    # corosync needs to be stop for the below to work.
    pcs cluster stop --all
    sleep 5s
    pcs quorum update wait_for_all=0
    sleep 25s
    pcs cluster start --all
    # heuristics change not applied - Hard to validate if it makes a difference to quorum decision making.
    ##pcs quorum device update heuristics mode=on exec_CUSTOM=/root/heuristics.sh
    # good to do a final restart of cluster services
    ##sleep 10s
    ##pcs cluster stop --all
    ##sleep 25s
    ##pcs cluster start --all

  else
    # for node1 to finish the configuration
    echo "Waiting for node1 to finish configuring PCS. You can run pcs status to see the latest status"
    sleep 60s
  fi





